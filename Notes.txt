Github issue containing correspondence with the original authors:
https://github.com/Microsoft/graph-based-code-modelling/issues/2

Notes: 4/22
- TLDR: Trying first to get tensorise.py to work, it isn't

Going to:
1) Set up env
2) Try running the tensorise command above
3) do it to every training dataset

openlivewriter doesn't have train/test split - removed it 
Need to run "export CUDA_VISIBLE_DEVICES=0" on turing to avoid consuming all of gpu memory
Have to convert program graphs from .gz to .json.gz (RichPath module doesn't support other file types)
- Will do this in batches via script or command line

python Models/utils/tensorise.py --hypers-override '{"cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --debug --model graph2seq data/tensorized ./graph-dataset/polly/polly-typehierarchy.json.gz ./graph-dataset/polly/graphs-train ./graph-dataset/polly/graphs-valid ./graph-dataset/polly/graphs-test

python Models/utils/tensorise.py --hypers-override '{"cg_add_subtoken_nodes" : false} --model graph2seq data/tensorized ./graph-dataset/polly/polly-typehierarchy.json.gz ./graph-dataset/polly/graphs-train ./graph-dataset/polly/graphs-valid ./graph-dataset/polly/graphs-test

Process Process-1:
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "Models/utils/../exprsynth/utils.py", line 75, in __parallel_queue_worker
    for result in worker_fn(worker_id, job):
  File "Models/utils/../exprsynth/model.py", line 255, in metadata_parser_fn
    type(self)._load_metadata_from_sample(self.hyperparameters, raw_sample=raw_sample, raw_metadata=raw_metadata)
  File "Models/utils/../exprsynth/graph2seqmodel.py", line 65, in _load_metadata_from_sample
    super(Graph2SeqModel, Graph2SeqModel)._load_metadata_from_sample(hyperparameters, raw_sample, raw_metadata)
  File "Models/utils/../exprsynth/contextgraphmodel.py", line 287, in _load_metadata_from_sample
    for edge_type, values in raw_sample['ContextGraph']['EdgeValues'].items():
KeyError: 'EdgeValues'

Sample representation printed here - looks exactly like the program graph.  Turns out edges can have values (weights)
OrderedDict([('filename', 'src\\Polly.Shared\\PolicyResult.cs'), ('slotTokenIdx', 29), ('ContextGraph', OrderedDict([('Edges', OrderedDict([('Child', [[3, 4], [3, 5], [3, 0], [6, 3], [6, 7], [8, 9], [8, 10], [8, 11], [12, 13], [12, 14], [12, 15], [16, 17], [18, 19], [20, 21], [20, 18], [20, 22], [20, 23], [20, 24], [20, 25], [20, 26], [20, 27], [20, 28], [23, 29], [30, 31], [32, 33], [32, 34], [32, 35], [32, 16], [32, 36], [32, 30], [32, 37], [32, 38], [32, 39], [32, 40], [32, 41], [42, 43], [44, 45], [44, 46], [44, 47], [44, 48], [44, 49], [44, 42], [44, 50], [44, 51], [44, 52], [44, 53], [44, 54], [44, 55], [44, 56], [44, 57], [44, 58], [48, 59], [60, 44], [61, 32], [62, 20], [62, 63], [64, 65], [64, 66], [64, 67], [68, 64], [68, 69], [11, 70], [11, 71], [11, 72], [63, 73], [63, 6], [63, 74], [75, 76], [75, 77], [75, 78], [75, 79], [75, 62], [75, 8], [75, 80], [75, 81], [75, 82], [75, 83], [75, 84], [75, 85], [86, 75], [86, 87], [87, 88], [87, 89], [87, 90], [87, 91], [87, 92], [87, 61], [87, 60], [87, 12], [87, 93], [87, 94], [87, 95], [87, 96], [87, 97], [87, 98], [87, 99], [87, 100], [87, 101], [87, 102], [15, 103], [15, 104], [15, 105], [104, 106], [107, 108], [71, 109]]), ('NextToken', [[4, 5], [5, 0], [0, 7], [7, 110], [8, 70], [65, 66], [12, 103], [16, 36], [17, 16], [36, 31], [18, 22], [19, 18], [22, 29], [21, 19], [23, 24], [25, 26], [27, 28], [28, 73], [29, 23], [30, 37], [31, 30], [34, 35], [35, 17], [38, 39], [40, 41], [42, 50], [43, 42], [46, 47], [47, 59], [48, 49], [49, 43], [51, 52], [53, 54], [55, 56], [57, 58], [59, 48], [111, 108], [67, 69], [69, 112], [66, 67], [9, 10], [10, 8], [70, 109], [73, 4], [74, 9], [76, 77], [77, 78], [78, 79], [85, 88], [88, 89], [89, 90], [13, 14], [14, 12], [103, 106], [113, 13], [108, 65]]), ('LastUse', [[4, 2], [110, 80], [112, 93], [65, 12], [1, 18], [114, 23], [115, 116], [117, 118], [119, 27], [120, 121], [122, 25], [123, 42], [124, 57], [125, 55], [126, 53], [127, 51], [111, 46], [67, 48], [2, 8], [128, 81], [129, 82], [130, 94], [131, 95], [132, 96], [133, 97], [134, 98], [135, 136]]), ('LastWrite', [[110, 80], [112, 93], [65, 12], [1, 18], [114, 23], [115, 116], [117, 118], [119, 27], [120, 121], [122, 25], [123, 42], [124, 57], [125, 55], [126, 53], [127, 51], [111, 46], [67, 48], [2, 8], [128, 81], [129, 82], [130, 94], [131, 95], [132, 96], [133, 97], [134, 98], [135, 136]]), ('LastLexicalUse', [[80, 110], [112, 80], [93, 112], [8, 2], [65, 8], [12, 65], [16, 1], [1, 18], [114, 23], [30, 114], [38, 115], [40, 117], [42, 30], [46, 34], [48, 16], [51, 38], [57, 40], [115, 116], [117, 118], [118, 120], [116, 122], [119, 27], [121, 119], [120, 121], [122, 25], [123, 42], [124, 57], [137, 124], [125, 55], [126, 53], [127, 51], [138, 127], [111, 46], [136, 111], [67, 48], [2, 4], [81, 128], [82, 129], [94, 130], [95, 131], [96, 132], [97, 133], [98, 134], [130, 81], [134, 82], [135, 136]])])), ('NodeLabels', OrderedDict([('0', '<SLOT>'), ('1', 'outcome'), ('2', 'Outcome'), ('3', 'SimpleAssignmentExpression'), ('4', 'Outcome'), ('5', '='), ('6', 'ExpressionStatement'), ('7', ';'), ('8', 'Outcome'), ('9', 'public'), ('10', 'OutcomeType'), ('11', 'AccessorList'), ('12', 'Outcome'), ('13', 'public'), ('14', 'OutcomeType'), ('15', 'AccessorList'), ('16', 'outcome'), ('17', 'OutcomeType'), ('18', 'outcome'), ('19', 'OutcomeType'), ('20', 'ParameterList'), ('21', '('), ('22', ','), ('23', 'finalException'), ('24', ','), ('25', 'exceptionType'), ('26', ','), ('27', 'context'), ('28', ')'), ('29', 'Exception'), ('30', 'finalException'), ('31', 'Exception'), ('32', 'ParameterList'), ('33', '('), ('34', 'result'), ('35', ','), ('36', ','), ('37', ','), ('38', 'exceptionType'), ('39', ','), ('40', 'context'), ('41', ')'), ('42', 'finalException'), ('43', 'Exception'), ('44', 'ParameterList'), ('45', '('), ('46', 'result'), ('47', ','), ('48', 'outcome'), ('49', ','), ('50', ','), ('51', 'exceptionType'), ('52', ','), ('53', 'finalHandledResult'), ('54', ','), ('55', 'faultType'), ('56', ','), ('57', 'context'), ('58', ')'), ('59', 'OutcomeType'), ('60', 'ConstructorDeclaration'), ('61', 'ConstructorDeclaration'), ('62', 'ConstructorDeclaration'), ('63', 'Block'), ('64', 'SimpleAssignmentExpression'), ('65', 'Outcome'), ('66', '='), ('67', 'outcome'), ('68', 'ExpressionStatement'), ('69', ';'), ('70', '{'), ('71', 'GetAccessorDeclaration'), ('72', '}'), ('73', '{'), ('74', '}'), ('75', 'ClassDeclaration'), ('76', 'public'), ('77', 'class'), ('78', 'PolicyResult'), ('79', '{'), ('80', 'FinalException'), ('81', 'ExceptionType'), ('82', 'Context'), ('83', 'MethodDeclaration'), ('84', 'MethodDeclaration'), ('85', '}'), ('86', 'NamespaceDeclaration'), ('87', 'ClassDeclaration'), ('88', 'public'), ('89', 'class'), ('90', 'PolicyResult'), ('91', 'TypeParameterList'), ('92', '{'), ('93', 'FinalException'), ('94', 'ExceptionType'), ('95', 'Result'), ('96', 'FinalHandledResult'), ('97', 'FaultType'), ('98', 'Context'), ('99', 'MethodDeclaration'), ('100', 'MethodDeclaration'), ('101', 'MethodDeclaration'), ('102', '}'), ('103', '{'), ('104', 'GetAccessorDeclaration'), ('105', '}'), ('106', 'get'), ('107', 'ExpressionStatement'), ('108', ';'), ('109', 'get'), ('110', 'FinalException'), ('111', 'result'), ('112', 'FinalException'), ('113', '}'), ('114', 'finalException'), ('115', 'exceptionType'), ('116', 'exceptionType'), ('117', 'context'), ('118', 'context'), ('119', 'context'), ('120', 'context'), ('121', 'context'), ('122', 'exceptionType'), ('123', 'finalException'), ('124', 'context'), ('125', 'faultType'), ('126', 'finalHandledResult'), ('127', 'exceptionType'), ('128', 'ExceptionType'), ('129', 'Context'), ('130', 'ExceptionType'), ('131', 'Result'), ('132', 'FinalHandledResult'), ('133', 'FaultType'), ('134', 'Context'), ('135', 'result'), ('136', 'result'), ('137', 'context'), ('138', 'exceptionType')])), ('NodeTypes', OrderedDict([('1', 'Polly.OutcomeType'), ('2', 'Polly.OutcomeType'), ('4', 'Polly.OutcomeType'), ('8', 'Polly.OutcomeType'), ('9', 'Polly.OutcomeType'), ('12', 'Polly.OutcomeType'), ('13', 'Polly.OutcomeType'), ('16', 'Polly.OutcomeType'), ('18', 'Polly.OutcomeType'), ('23', 'System.Exception'), ('25', 'Polly.ExceptionType?'), ('27', 'Polly.Context'), ('30', 'System.Exception'), ('34', 'TResult'), ('38', 'Polly.ExceptionType?'), ('40', 'Polly.Context'), ('42', 'System.Exception'), ('46', 'TResult'), ('48', 'Polly.OutcomeType'), ('51', 'Polly.ExceptionType?'), ('53', 'TResult'), ('55', 'Polly.FaultType?'), ('57', 'Polly.Context'), ('60', 'void'), ('61', 'void'), ('62', 'void'), ('65', 'Polly.OutcomeType'), ('67', 'Polly.OutcomeType'), ('71', 'Polly.OutcomeType'), ('80', 'System.Exception'), ('81', 'Polly.ExceptionType?'), ('82', 'Polly.Context'), ('83', 'Polly.PolicyResult'), ('84', 'Polly.PolicyResult'), ('93', 'System.Exception'), ('94', 'Polly.ExceptionType?'), ('95', 'TResult'), ('96', 'TResult'), ('97', 'Polly.FaultType?'), ('98', 'Polly.Context'), ('99', 'Polly.PolicyResult<TResult>'), ('100', 'Polly.PolicyResult<TResult>'), ('101', 'Polly.PolicyResult<TResult>'), ('104', 'Polly.OutcomeType'), ('106', 'Polly.OutcomeType'), ('109', 'Polly.OutcomeType'), ('110', 'System.Exception'), ('111', 'TResult'), ('112', 'System.Exception'), ('114', 'System.Exception'), ('115', 'Polly.ExceptionType'), ('116', 'Polly.ExceptionType'), ('117', 'Polly.Context'), ('118', 'Polly.Context'), ('119', 'Polly.Context'), ('120', 'Polly.Context'), ('121', 'Polly.Context'), ('122', 'Polly.ExceptionType?'), ('123', 'System.Exception'), ('124', 'Polly.Context'), ('125', 'Polly.FaultType?'), ('126', 'TResult'), ('127', 'Polly.ExceptionType?'), ('128', 'Polly.ExceptionType?'), ('129', 'Polly.Context'), ('130', 'Polly.ExceptionType?'), ('131', 'TResult'), ('132', 'TResult'), ('133', 'Polly.FaultType?'), ('134', 'Polly.Context'), ('135', 'TResult'), ('136', 'TResult'), ('137', 'Polly.Context'), ('138', 'Polly.ExceptionType')]))])), ('SlotDummyNode', 0), ('SymbolCandidates', [OrderedDict([('SymbolDummyNode', 1), ('SymbolName', 'outcome'), ('IsCorrect', True)]), OrderedDict([('SymbolDummyNode', 2), ('SymbolName', 'Outcome'), ('IsCorrect', False)])])])
{'type_lattice_path': ./graph-dataset/polly/polly-typehierarchy.json.gz, 'cg_node_type_counter': Counter(), 'decoder_token_counter': Counter(), 'cg_node_label_counter': Counter(), 'cg_edge_value_sizes': {}, 'cg_edge_types': set()}


Notes 4/23:

See github issue for mmjb@'s response.  Edge values don't matter in our case, so we can just set the dict to be empty.

Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "Models/utils/../exprsynth/utils.py", line 75, in __parallel_queue_worker
    for result in worker_fn(worker_id, job):
  File "Models/utils/../exprsynth/model.py", line 255, in metadata_parser_fn
    type(self)._load_metadata_from_sample(self.hyperparameters, raw_sample=raw_sample, raw_metadata=raw_metadata)
  File "Models/utils/../exprsynth/graph2seqmodel.py", line 66, in _load_metadata_from_sample
    SeqDecoder.load_metadata_from_sample(raw_sample, raw_metadata)
  File "Models/utils/../exprsynth/seqdecoder.py", line 246, in load_metadata_from_sample
    symbol_id_to_label = raw_sample['SymbolLabels']
KeyError: 'SymbolLabels'

g['SymbolLabels']
{'4': '[', '6': ']', '3': 'args', '7': '0'}
  - Looks like its just the tokens corresponding to the hole/slot they're trying to complete?

Look at the following methods to see if they do anything that contradicts our changes:
1) Test time: model.test()
- loaded_train_sample['Provenance'] = raw_sample['Filename'] + "::" + raw_sample['HoleLineSpan']
- prod_root_node = min(int(v) for v in raw_sample['Productions'].keys())
  sample_token_seq = []
  collect_token_seq(raw_sample, prod_root_node, sample_token_seq)
    • raw_sample['Productions'] --> {'1': [2, 4, 5, 6], '2': [3], '5': [7]}
    • Seems like productions is an artifact from how they expand the AST
- if context_encoding is None:  # TODO: Hack that should go away
    • what's the hack here?
2) load_data_from_sample (graph2seq contextgraph and seqdecoder)
- in SeqDecoder: uses the Productions element of the context graph (in the first 3 lines) to generate the actual tokens
- in ContextGraphModel: calls __load_contextgraph_data_from_sample

Other issues:
- Replace <HOLE> with <SLOT> as the keyword in the vocabulary (they had different dataset formats and in the second paper they used <HOLE> instead.  Probably change it when loading data)
- change 'HoleNode' to 'SlotDummyNode' (this is a result of the change above)

Conversation with Michele:
- We should come up with an interesting (but not necessarily sophisticated) baseline.  E.g. even randomly guessing from the space of possible valid completions could do pretty well.  Look at a non-DL method
- Error analysis: how does their model perform when there are many possible solutions (vs. only 1)
- Model of work a bit, then ping them when I get stuck is good.
- Look into code2seq once I get the chance/if I get bored here.
- Make a 1P doc on SCUBA (what the web presence should look like) before EOW.
- Using the character CNN as input is totally fine (want to be replecating state of the art, but should also look at all tokend)
- I should read about CharCNNs
- Holding off on making decisions about dataset: wait until we see reproducibility of graphical models across PLs (i.e. dynamic/static typed).

4/24

Made the revisions found on 4/23 and followed up with suggested edits from the GitHub issue
Tensorising works! (I think)
  - Now have tensorized graphs polly/graphs-train, polly/graphs-valid, polly/graphs-test into the directories data/tensorized/graphs-____ 

python Models/utils/tensorise.py --hypers-override '{"cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --debug --model graph2seq data/tensorized ./graph-dataset/polly/polly-typehierarchy.json.gz ./graph-dataset/polly/graphs-train ./graph-dataset/polly/graphs-valid git 

Not sure whether or not I included the residual connections... oops

4/29
Trying to overfit this to the polly dataset

"""
Usage:
    train.py [options] SAVE_FOLDER TRAIN_DATA_PATH VALID_DATA_PATH

Options:
    -h --help                  Show this screen.
    --max-num-epochs EPOCHS    The maximum number of epochs to run [default: 300]
    --hypers-override HYPERS   JSON dictionary overriding hyperparameter values.
    --model MODELNAME          Choose model type. [default: NAG]
    --run-name NAME            Picks a name for the trained model.
    --quiet                    Less output (not one per line per minibatch). [default: False]
    --azure-info=<path>        Azure authentication information file (JSON). Used to load data from Azure storage.
    --debug                    Enable debug routines. [default: False]
"""

run from base directory:
python Models/utils/train.py --model graph2seq --hypers-override '{"patience": 150, "cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --run-name 4-29-polly --max-num-epochs 150 Models/trained_models data/tensorized/graphs-train data/tensorized/graphs-valid

Traceback (most recent call last):
  File "Models/utils/train.py", line 101, in <module>
    run_and_debug(lambda: run(args), args.get('--debug', False))
  File "/dfs/scratch1/haighal/haighal/lib/python3.5/site-packages/dpu_utils/utils/debughelper.py", line 11, in run_and_debug
    func()
  File "Models/utils/train.py", line 101, in <lambda>
    run_and_debug(lambda: run(args), args.get('--debug', False))
  File "Models/utils/train.py", line 96, in run
    arguments.get('--run-name'), arguments.get('--quiet', False))
  File "Models/utils/train.py", line 58, in run_train
    model_path = model.train(train_data_chunk_paths, valid_data_chunk_paths, quiet=quiet, resume=resume)
  File "Models/utils/../exprsynth/model.py", line 554, in train
    train_loss = self.__run_epoch_in_batches(train_data, "%i (train)" % (epoch_number,), is_train=True, quiet=quiet)
  File "Models/utils/../exprsynth/model.py", line 505, in __run_epoch_in_batches
    for minibatch_counter, (batch_data_dict, samples_in_batch, samples_used_so_far) in enumerate(data_generator):
  File "Models/utils/../exprsynth/model.py", line 487, in _data_to_minibatches
    minibatch = self._finalise_minibatch(raw_batch, is_train)
  File "Models/utils/../exprsynth/graph2seqmodel.py", line 107, in _finalise_minibatch
    self._decoder_model.finalise_minibatch(batch_data, minibatch)
TypeError: finalise_minibatch() missing 1 required positional argument: 'is_train'

^ Just removed is_train argument on SeqDecoder.finalise_minibatch() since it isn't used.

Set max_num_epochs to 150, patience to 150 so that we actually overfit the sample dataset:
python Models/utils/train.py --model graph2seq --hypers-override '{"patience": 150, "max_epochs":150, "cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --run-name polly Models/trained_models data/tensorized/graphs-train data/tensorized/graphs-valid

==== Epoch 0 ====
  Epoch 0 (train) took 21.44s [processed 104 samples/second]
 Training Loss: 16.507815
  Epoch 0 (valid) took 1.44s [processed 270 samples/second]
 Validation Loss: 14.557160
  Best result so far -- saving model as './polly_polly_model_best.pkl.gz'.
==== Epoch 1 ====
  Epoch 1 (train) took 17.29s [processed 130 samples/second]
 Training Loss: 12.512107
  Epoch 1 (valid) took 1.47s [processed 265 samples/second]
 Validation Loss: 12.847222
  Best result so far -- saving model as './polly_polly_model_best.pkl.gz'.
.
.
.
==== Epoch 148 ====
  Epoch 148 (train) took 14.97s [processed 150 samples/second]
 Training Loss: 1.638676
  Epoch 148 (valid) took 1.41s [processed 277 samples/second]
 Validation Loss: 13.763917
==== Epoch 149 ====
  Epoch 149 (train) took 15.42s [processed 145 samples/second]
 Training Loss: 1.630338
  Epoch 149 (valid) took 1.51s [processed 259 samples/second]
 Validation Loss: 13.854025


TEST-TIME
python Models/utils/test.py ./polly_polly_model_best.pkl.gz ./graph-dataset/polly/graphs-test Models/trained_models/polly

"""
Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 47, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
  File "Models/utils/test.py", line 82, in test_on_raw_chunks
    num_samples = model.test(test_raw_data_chunks, per_result_callback_fn=per_result_callback, train_model=train_model)
  File "Models/utils/../exprsynth/model.py", line 663, in test
    per_result_callback_fn(sample_idx, token_perplexity, raw_sample, test_result)
  File "Models/utils/test.py", line 76, in per_result_callback
    write_snippet(sample_idx, build_csharp_check_function(raw_sample, ' '.join(predictions[0][0])))
  File "Models/utils/test.py", line 33, in build_csharp_check_function
    return_type = sample_types['0'] if sample_types['0'] != '?' else 'object'
KeyError: '0'
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "Models/utils/test.py", line 130, in <module>
    run_and_debug(lambda: run(args), args.get('--debug', False))
  File "/dfs/scratch1/haighal/haighal/lib/python3.5/site-packages/dpu_utils/utils/debughelper.py", line 11, in run_and_debug
    func()
  File "Models/utils/test.py", line 130, in <lambda>
    run_and_debug(lambda: run(args), args.get('--debug', False))
  File "Models/utils/test.py", line 125, in run
    run_test(model_path, test_folder, output_folder, num_processes)
  File "Models/utils/test.py", line 103, in run_test
    num_samples, token_perplexities, correct_at_1, correct_at_5 = zip(*pool.starmap(test_on_raw_chunks, test_jobs))
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 268, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File "/usr/lib/python3.5/multiprocessing/pool.py", line 608, in get
    raise self._value
KeyError: '0'

For now, just fixed bug by just commenting out the snippet

*** Training Actual Model Now ***
1)  Renamed files (were .gz vs. .json.gz) and recreated train/dev/test split from ICLR '18
    Made rxnt a dev dataset because ravendb wasn't released.  Had to rename some files before running because they can't spell hierarchy
    python make-dataset.py 1

2)  Merged type lattices for all the training data samples
    python Models/utils/latticejoin.py graph-dataset/reorganized/type-hierarchies graph-dataset/reorganized/typehierarchy.json.gz

3)  Tensorize all the training and dev data
    python Models/utils/tensorise.py --hypers-override '{"cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --debug --model graph2seq data/full-tensorized graph-dataset/reorganized/typehierarchy.json.gz ./graph-dataset/reorganized/graphs-train ./graph-dataset/reorganized/graphs-valid ./graph-dataset/reorganized/graphs-test/seen ./graph-dataset/reorganized/graphs-test/unseen

    Got a weird crash on the unseen test project (overflow on unsigned int), re-run the tensorization on just that part 
    python Models/utils/tensorise.py --hypers-override '{"cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --debug --model graph2seq --metadat-to-use data/full-tensorized/graphs-train/metadata.pkl.gz data/full-tensorized graph-dataset/reorganized/typehierarchy.json.gz ./graph-dataset/reorganized/graphs-test/unseen

  Traceback (most recent call last):
  File "/usr/lib/python3.5/multiprocessing/process.py", line 249, in _bootstrap
    self.run()
  File "/usr/lib/python3.5/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "Models/utils/../exprsynth/utils.py", line 76, in __parallel_queue_worker
    for result in worker_fn(worker_id, job):
  File "Models/utils/../exprsynth/model.py", line 92, in data_file_parser
    target_path.save_as_compressed_file(result_data)
  File "/dfs/scratch1/haighal/haighal/lib/python3.5/site-packages/dpu_utils/utils/richpath.py", line 290, in save_as_compressed_file
    pickle.dump(data, outfile)
  File "/usr/lib/python3.5/gzip.py", line 262, in write
    self.fileobj.write(self.compress.compress(data))

4)  Train model
    python Models/utils/train.py --model graph2seq --hypers-override '{"cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' --run-name full_dataset_no_residual Models/trained_models data/full-tensorized/graphs-train data/full-tensorized/graphs-valid

5/02:

Test on seen projects:

python Models/utils/test.py Models/trained_models/full_dataset_no_residual_model_best.pkl.gz graph-dataset/graphs-train Models/trained_models/full_dataset_no_residual

python Models/utils/test.py Models/trained_models/full_dataset_no_residual_model_best.pkl.gz graph-dataset/graphs-valid Models/trained_models/full_dataset_no_residual

python Models/utils/test.py Models/trained_models/full_dataset_no_residual_model_best.pkl.gz graph-dataset/graphs-test/seen Models/trained_models/full_dataset_no_residual

python Models/utils/test.py Models/trained_models/full_dataset_no_residual_model_best.pkl.gz graph-dataset/graphs-test/unseen Models/trained_models/full_dataset_no_residual

5/06

Might have to refactor the dataset - see @mmjb's comment from 5/6 (they truncate the graphs at 8 hops away because they only use 1 hole for VarMisuse).  VarNaming they mask all instances of the variable name

re-tested after moving the dataset
retraining now on more GPUs
python Models/utils/train.py --model graph2seq --hypers-override '{"patience" : 20, "cg_add_subtoken_nodes" : false, "cg_ggnn_residual_connections" : {}}' Models/trained_models graph-dataset/tensorized/graphs-train graph-dataset/tensorized/graphs-valid
for testing - use the flag --num_processes 4 to speed it up

5/09

Testing takes forever - will ask about it.  Been running for 2 days on a GPU with 4 parallel workers and still not done.
Why?
  - Looks like model.test is very inefficient.  Why is `self._load_data_from_sample` called twice?  Once with is_train = True and once with is_train = False
  - Is it the beam search part that's slowing it down? Why do they check if context_encoding is None? (L648)  
      graph2seq model returns None for that part...

5/10
- trying to test again with the retrained model:

python Models/utils/test.py --num-processes 4 Models/trained_models/Graph2SeqModel_graph2seq-2019-05-07-11-50-17_model_best.pkl.gz graph-dataset/graphs-test/seen Models/trained_models/Graph2SeqModel_seen

- This time, was in directory /lfs/local/0/haighal/graph-based-code-modelling.  Seems to be running faster.  If not, try more workers